{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd05751d7c496c272386542b26abd80dd065779a39feb74bf22f2b4e5e7f629ef29",
   "display_name": "Python 3.7.10 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "5751d7c496c272386542b26abd80dd065779a39feb74bf22f2b4e5e7f629ef29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### General Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "import os\n",
    "import glob\n",
    "from cleanUp import cleanUp\n",
    "from fillDf import fillDf\n",
    "from fixYearStamp import fixYearStamp\n"
   ]
  },
  {
   "source": [
    "### Data Cleaning\n",
    "Passing the sensor data through the cleanUp function to get fix timestamps and delete null timestamps."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_csv_files = sorted(glob.glob(\"./Data/*.txt\"))\n",
    "# insert the desired start time\n",
    "cutOffTime = '4/19/2021 17:00:00'\n",
    "endTime = '4/19/2021 20:00:00'\n",
    "# insert the time rectifying offsets. default of for nothing {'':0}\n",
    "sensorConditions = {'S-01':7,'S-02':7,'S-03':7,'S-04':7,'S-05':7,'S-06':7,'S-15':7,'S-19':7}\n",
    "#This indicates which columns to keep. Here we're taking all of the dP info and the timestamps\n",
    "columns = [0,1,6,7,8,9,10,11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['./Data/S-01.txt',\n",
       " './Data/S-02.txt',\n",
       " './Data/S-03.txt',\n",
       " './Data/S-04.txt',\n",
       " './Data/S-05.txt',\n",
       " './Data/S-06.txt',\n",
       " './Data/S-07.txt',\n",
       " './Data/S-08.txt',\n",
       " './Data/S-09.txt',\n",
       " './Data/S-10.txt',\n",
       " './Data/S-11.txt',\n",
       " './Data/S-12.txt',\n",
       " './Data/S-13.txt',\n",
       " './Data/S-14.txt',\n",
       " './Data/S-15.txt',\n",
       " './Data/S-16.txt',\n",
       " './Data/S-18.txt',\n",
       " './Data/S-19.txt']"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "all_csv_files"
   ]
  },
  {
   "source": [
    "Changed this to markdown so it won't run twice, had to fix the timestamps on S-12\n",
    "filePath        = all_csv_files[11]\n",
    "incorrectString = '21/3/22'\n",
    "date            = '3/22/2021'\n",
    "charTimeStart   = 11\n",
    "charTimeEnd     = 21\n",
    "offset          = 0\n",
    "fixYearStamp(filePath,incorrectString,date,charTimeStart,charTimeEnd,offset)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "S-01     2021-04-19 17:22:10      2021-04-19 19:58:59\nS-02     2021-04-19 17:03:05      2021-04-19 19:59:29\nS-03     2021-04-19 17:19:10      2021-04-19 19:59:09\nS-04     2021-04-19 17:39:10      2021-04-19 19:58:59\nS-05     2021-04-19 17:06:10      2021-04-19 19:58:29\nS-06     2021-04-19 17:11:10      2021-04-19 19:59:09\nS-07     2021-04-19 17:07:34      2021-04-19 19:59:18\nS-08     2021-04-19 17:05:08      2021-04-19 19:59:11\nS-09     2021-04-19 17:10:11      2021-04-19 19:58:58\nS-10     2021-04-19 17:09:59      2021-04-19 19:58:36\nS-11     2021-04-19 17:19:22      2021-04-19 19:59:09\nS-12     2021-04-19 17:11:50      2021-04-19 19:58:57\nS-13     2021-04-19 17:21:07      2021-04-19 19:59:14\nS-14     2021-04-19 17:05:18      2021-04-19 19:59:15\nS-15     2021-04-19 17:12:10      2021-04-19 20:01:55\nS-16     2021-04-19 17:07:36      2021-04-19 19:58:50\nS-18     2021-04-19 17:11:07      2021-04-19 20:02:02\nS-19     2021-04-19 17:10:16      2021-04-19 20:02:01\n"
     ]
    }
   ],
   "source": [
    "data = cleanUp(cutOffTime,sensorConditions,all_csv_files,columns)"
   ]
  },
  {
   "source": [
    "### Exporting Data\n",
    "Here we can export the organized data frames as csv files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './proccessedData'\n",
    "for x in data:\n",
    "    temp=data[x]\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  },
  {
   "source": [
    "### Checking Data\n",
    "Here we scan through the data for irregularities in data recording."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.21 % potential error in  S-01\n",
      "   9  20\n",
      "   1   1\n",
      "\n",
      "0.09 % potential error in  S-02\n",
      "  14\n",
      "   1\n",
      "\n",
      "0.21 % potential error in  S-03\n",
      "   9  20\n",
      "   1   1\n",
      "\n",
      "0.6 % potential error in  S-04\n",
      "  11   9\n",
      "   2   3\n",
      "\n",
      "0.68 % potential error in  S-05\n",
      "   9  16   4  30  40  20\n",
      "   1   1   1   1   1   2\n",
      "\n",
      "0.2 % potential error in  S-06\n",
      "   9  20\n",
      "   1   1\n",
      "\n",
      "19.9 % potential error in  S-07\n",
      " 627  27  20\n",
      "   1   1 159\n",
      "\n",
      "49.86 % potential error in  S-08\n",
      "  27  59  20\n",
      "   2   1 343\n",
      "\n",
      "50.0 % potential error in  S-09\n",
      "  21  22  11  18  19  26  20\n",
      "   2   1   1   1   2   1 330\n",
      "\n",
      "49.85 % potential error in  S-10\n",
      "  17  20\n",
      "   1 336\n",
      "\n",
      "19.88 % potential error in  S-11\n",
      "  27  20\n",
      "   1 158\n",
      "\n",
      "99.8 % potential error in  S-12\n",
      "  27  20\n",
      "   1 500\n",
      "\n",
      "20.08 % potential error in  S-13\n",
      "  27  23  19  58  20\n",
      "   1   1   1   1 154\n",
      "\n",
      "49.92 % potential error in  S-14\n",
      " 939  27  21  20\n",
      "   1   1   1 314\n",
      "\n",
      "0.49 % potential error in  S-15\n",
      "   9  17  15  11  13\n",
      "   1   1   1   1   1\n",
      "\n",
      "0.1 % potential error in  S-16\n",
      "  14\n",
      "   1\n",
      "\n",
      "0.39 % potential error in  S-18\n",
      "  17   9  14  15\n",
      "   1   1   1   1\n",
      "\n",
      "0.1 % potential error in  S-19\n",
      "  15\n",
      "   1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "directory = './dataInfo'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "fout = open('./dataInfo/time_Frequency_Error_Log.txt','wt')\n",
    "errors = {}\n",
    "errorCount = {}\n",
    "# Enter the expected interval here\n",
    "interval = 10\n",
    "for x in data:\n",
    "    # errors keeps track of length of each time interval error that occurs\n",
    "    errors[x] = set(())\n",
    "    # errorCount keeps track of how many times each time interval error occured\n",
    "    errorCount[x] = {}\n",
    "    # counter keeps track of the total time interval errors per sensor\n",
    "    counter = 0\n",
    "    #shows the total\n",
    "    temp = data[x]\n",
    "    for idx,i in enumerate(temp['Date_Time']):\n",
    "        try:\n",
    "            if not ((temp['Date_Time'][idx+1] - i) == pd.Timedelta(seconds=interval)):\n",
    "                timeErr = temp['Date_Time'][idx+1] - i\n",
    "                if str(timeErr.seconds) in errorCount[x]:\n",
    "                    errorCount[x][str(timeErr.seconds)] +=1\n",
    "                else:\n",
    "                    errorCount[x][str(timeErr.seconds)] = 1\n",
    "\n",
    "                errors[x].add(timeErr)\n",
    "\n",
    "\n",
    "                counter += 1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(str(round(counter/len(temp)*100,2)),'% potential error in ', x)\n",
    "    fout.write('potential error in '+ x +'\\n' + str(round(counter/len(temp)*100,2))+'%'+'\\n')\n",
    "\n",
    "    # display the different types of errors\n",
    "    lst = [i.seconds for i in errors[x]]\n",
    "    frmt = \"{:>4}\"*len(lst)\n",
    "    print(frmt.format(*lst))\n",
    "    fout.write(\"Time Errors\" + frmt.format(*lst)+ '\\n')\n",
    "\n",
    "    # display the quantity of each type of error\n",
    "    lst = [errorCount[x][str(i.seconds)] for i in errors[x]]\n",
    "    frmt = \"{:>4}\"*len(lst)\n",
    "    print(frmt.format(*lst))\n",
    "    fout.write(\"# Observed \" + frmt.format(*lst)+ '\\n')\n",
    "\n",
    "    print()\n",
    "    fout.write('\\n')\n",
    "\n",
    "\n",
    "fout.close()"
   ]
  },
  {
   "source": [
    "Notice there are quite a few repeating errors here in our data set. We can either choose to interpolate the data inbetween or pad it with 0s. For gaps <40s i will interpolate, but for gaps >40 i will 0 pad."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "S-01   ['% of values from interpolation : 0.186', '% of values from 0-padding : 12.477', '% of values not changed : 87.337']\n",
      "S-02   ['% of values from interpolation : 0.0', '% of values from 0-padding : 1.764', '% of values not changed : 98.236']\n",
      "S-03   ['% of values from interpolation : 0.186', '% of values from 0-padding : 10.791', '% of values not changed : 89.023']\n",
      "S-04   ['% of values from interpolation : 0.372', '% of values from 0-padding : 21.974', '% of values not changed : 77.654']\n",
      "S-05   ['% of values from interpolation : 1.027', '% of values from 0-padding : 3.548', '% of values not changed : 95.425']\n",
      "S-06   ['% of values from interpolation : 0.186', '% of values from 0-padding : 6.326', '% of values not changed : 93.488']\n",
      "S-07   ['% of values from interpolation : 29.74', '% of values from 0-padding : 10.13', '% of values not changed : 60.13']\n",
      "S-08   ['% of values from interpolation : 64.312', '% of values from 0-padding : 3.439', '% of values not changed : 32.249']\n",
      "S-09   ['% of values from interpolation : 62.663', '% of values from 0-padding : 5.773', '% of values not changed : 31.564']\n",
      "S-10   ['% of values from interpolation : 62.873', '% of values from 0-padding : 5.597', '% of values not changed : 31.53']\n",
      "S-11   ['% of values from interpolation : 29.581', '% of values from 0-padding : 10.884', '% of values not changed : 59.535']\n",
      "S-12   ['% of values from interpolation : 93.296', '% of values from 0-padding : 6.704', '% of values not changed : 0.0']\n",
      "S-13   ['% of values from interpolation : 29.275', '% of values from 0-padding : 12.361', '% of values not changed : 58.364']\n",
      "S-14   ['% of values from interpolation : 58.829', '% of values from 0-padding : 11.71', '% of values not changed : 29.461']\n",
      "S-15   ['% of values from interpolation : 0.185', '% of values from 0-padding : 6.852', '% of values not changed : 92.963']\n",
      "S-16   ['% of values from interpolation : 0.186', '% of values from 0-padding : 4.283', '% of values not changed : 95.531']\n",
      "S-18   ['% of values from interpolation : 0.37', '% of values from 0-padding : 6.204', '% of values not changed : 93.426']\n",
      "S-19   ['% of values from interpolation : 0.185', '% of values from 0-padding : 5.741', '% of values not changed : 94.074']\n"
     ]
    }
   ],
   "source": [
    "fout = open('./dataInfo/interpolation_Effect_Log.txt','wt')\n",
    "interpDF = {}\n",
    "\n",
    "for x in data:\n",
    "    df = data[x]\n",
    "    cutoff = 40\n",
    "    freq = '10S'\n",
    "    try:\n",
    "        interpDF[x],accuracy = fillDf(df,freq,cutOffTime,endTime,cutoff)\n",
    "        print(x,' ',accuracy)\n",
    "        fout.write(x+' '+ '\\n' + accuracy[0]+ '\\n'+ accuracy[1]+ '\\n'+ accuracy[2] +'\\n\\n')\n",
    "    except IndexError:\n",
    "        print(x,'NO DATA')\n",
    "        fout.write(x+'NO DATA'+'\\n')\n",
    "fout.close()        "
   ]
  },
  {
   "source": [
    "### Export Data\n",
    "export the newly interpolated data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './interpolatedData'\n",
    "for x in interpDF:\n",
    "    temp=interpDF[x]\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  },
  {
   "source": [
    "### Merge the DataFrames\n",
    "Also remove 'S-02' from the dictionary as it has no real data\n",
    "and find the least common index"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1071\n"
     ]
    }
   ],
   "source": [
    "# interpDF.pop('S-02',None)\n",
    "# interpDF.pop('S-BU2',None)\n",
    "# interpDF.pop('S-BU1',None)\n",
    "length = []\n",
    "for x in interpDF:\n",
    "    length.append(len(interpDF[x]))\n",
    "index = min(length)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             Date_Time  Dp>0.3  Dp>0.5  Dp>1.0  Dp>2.5  Dp>5.0  Dp>10.0\n",
       "15 2021-04-19 17:12:46     111      37       9       3       0        0\n",
       "16 2021-04-19 17:12:56      39      13       0       0       0        0\n",
       "17 2021-04-19 17:13:06      81      27       0       0       0        0\n",
       "18 2021-04-19 17:13:16     177      59       0       0       0        0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date_Time</th>\n      <th>Dp&gt;0.3</th>\n      <th>Dp&gt;0.5</th>\n      <th>Dp&gt;1.0</th>\n      <th>Dp&gt;2.5</th>\n      <th>Dp&gt;5.0</th>\n      <th>Dp&gt;10.0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>15</th>\n      <td>2021-04-19 17:12:46</td>\n      <td>111</td>\n      <td>37</td>\n      <td>9</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>2021-04-19 17:12:56</td>\n      <td>39</td>\n      <td>13</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>2021-04-19 17:13:06</td>\n      <td>81</td>\n      <td>27</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>2021-04-19 17:13:16</td>\n      <td>177</td>\n      <td>59</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "tempList = temp[15:19]\n",
    "tempList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "1",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b1a692eb069e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterpDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "for count,key in enumerate(list(interpDF.keys())):\n",
    "    print(count+1,key,temp[count+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMerged = []\n",
    "columns = list(interpDF.keys())\n",
    "columns.extend(['Average',\n",
    "'Variance'])\n",
    "# 'Zone 1',\n",
    "# 'Var Z1',\n",
    "# 'Zone 2',\n",
    "# 'Var Z2',\n",
    "# 'Zone 3',\n",
    "# 'Var Z3'])\n",
    "# 'Zone 4',\n",
    "# 'Var Z4'])\n",
    "\n",
    "for idx,i in enumerate(interpDF[columns[0]].values[:index]):\n",
    "    temp = []\n",
    "    temp.append(i[0])\n",
    "    for x in interpDF:\n",
    "        temp.append(interpDF[x].values[idx][1])\n",
    "    #So we now have a list with the timestamp and then sensors\n",
    "    \n",
    "    #here we add the overall average and variance columns\n",
    "    temp.append(np.average(temp[1:16]))\n",
    "    temp.append(np.std(temp[1:16]))\n",
    "\n",
    "    #here we're segregating the zones in the file giving their variance and avg\n",
    "\n",
    "    # #Zone 1 the 2 sensors right on top of the nebulizer\n",
    "    # lst = temp[1:7]\n",
    "    # temp.append(np.average(lst))\n",
    "    # temp.append(np.std(lst))\n",
    "    # # #Zone 2 the perimiter of the bed\n",
    "    # # lst = [temp[2],temp[3],temp[5],temp[6]]\n",
    "    # # temp.append(np.average(lst))\n",
    "    # # temp.append(np.std(lst))\n",
    "    # #Zone 3 the perimeter of the room\n",
    "    # lst = temp[7:16]\n",
    "    # temp.append(np.average(lst))\n",
    "    # temp.append(np.std(lst))\n",
    "    # #Zone 4 is just the outside sensor\n",
    "    # lst = temp[16:19]\n",
    "    # temp.append(np.average(lst))\n",
    "    # temp.append(np.std(lst))\n",
    "    dfMerged.append(temp)\n",
    "columns.insert(0,'Date_Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedData = pd.DataFrame(dfMerged,columns = columns)"
   ]
  },
  {
   "source": [
    "### Increase Resolution on mergedData"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in mergedData:\n",
    "    tempFrame = mergedData.values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    hiResMergedDF = pd.DataFrame(tempList, columns = mergedData.keys())"
   ]
  },
  {
   "source": [
    "### Export Merged Frames"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './mergedData/'\n",
    "if not os.path.exists(directory):\n",
    "\n",
    "    os.makedirs(directory)\n",
    "\n",
    "location = os.path.join(directory+'mergedFrame.csv')\n",
    "hiResMergedDF.to_csv(location,index=False)"
   ]
  },
  {
   "source": [
    "### Create csv files for each animation\n",
    "We have 3 expirements in each that we want to average across the range"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = '4/19/2021'\n",
    "expTRange = {\n",
    "\n",
    "    'OR 5 Unblocked':\n",
    "    [\n",
    "    pd.Timestamp(day + ' 5:23:24 PM'),\n",
    "    pd.Timestamp(day + ' 5:32:20 PM'),\n",
    "    pd.Timestamp(day + ' 5:42:00 PM'),\n",
    "    pd.Timestamp(day + ' 5:52:00 PM'),\n",
    "    pd.Timestamp(day + ' 5:58:00 PM'),\n",
    "    pd.Timestamp(day + ' 6:25:20 PM')],\n",
    "    ],\n",
    "    'OR 5 Blocked':\n",
    "    [\n",
    "    pd.Timestamp(day + ' 6:08:50 PM'),\n",
    "    pd.Timestamp(day + ' 6:16:50 PM'),\n",
    "    pd.Timestamp(day + ' 6:25:20 PM')],\n",
    "    'OR 12 Unblocked':\n",
    "    [\n",
    "    pd.Timestamp(day + ' 6:52:50 PM'),\n",
    "    pd.Timestamp(day + ' 7:03:30 PM'),\n",
    "    pd.Timestamp(day + ' 7:13:30 PM')],\n",
    "    'OR 12 Blocked':\n",
    "    [\n",
    "    pd.Timestamp(day + ' 7:25:24 PM'),\n",
    "    pd.Timestamp(day + ' 7:34:45 PM'),\n",
    "    pd.Timestamp(day + ' 7:38:24 PM')],\n",
    "}\n",
    "\n",
    "#enter in the expirement length as seconds/10\n",
    "expTLen = {\n",
    "    'OR 5 Unblocked' : 15*6,\n",
    "    'OR 5 Blocked':15*6,\n",
    "    'OR 12 Unblocked':10*6,\n",
    "    'OR 12 Blocked':10*6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mergedData = pd.read_csv('./mergedData/mergedFrame.csv',parse_dates=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time = mergedData['Date_Time']\n",
    "expIndexes = {}\n",
    "for i in expTRange:\n",
    "    expIndexes[i] = []\n",
    "    for x in expTRange[i]:\n",
    "        for start,n in enumerate(time):\n",
    "           if n >= x:\n",
    "               expIndexes[i].append(start)\n",
    "               break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "metadata": {},
     "execution_count": 268
    }
   ],
   "source": [
    "expTLen[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# controls how many seconds of data before each experiment to include\n",
    "preCursorFactor = 6\n",
    "averagedFrame = {}\n",
    "expirementFrame = {}\n",
    "\n",
    "for label in expIndexes:\n",
    "\n",
    "    df1Index1 = expIndexes[label][0] - preCursorFactor\n",
    "    df1Index2 = expIndexes[label][0] + expTLen[label]\n",
    "    df1 = mergedData.iloc[df1Index1 : df1Index2 , 1: ].reset_index(drop = True)\n",
    "\n",
    "    df2Index1 = expIndexes[label][1] - preCursorFactor\n",
    "    df2Index2 = expIndexes[label][1] + expTLen[label]\n",
    "    df2 = mergedData.iloc[df2Index1 : df2Index2 , 1: ].reset_index(drop = True)\n",
    "\n",
    "    df3Index1 = expIndexes[label][2] - preCursorFactor\n",
    "    df3Index2 = expIndexes[label][2] + expTLen[label]\n",
    "    df3 = mergedData.iloc[df3Index1 : df3Index2 , 1: ].reset_index(drop = True)\n",
    "\n",
    "    averagedFrame[label] = (df1 + df2 + df3)/3\n",
    "\n",
    "    expirementFrame[label+' Exp1'] = df1\n",
    "    expirementFrame[label+' Exp2'] = df2\n",
    "    expirementFrame[label+' Exp3'] = df3\n",
    "    \n",
    "#assuming there were 3 expirements for each one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './averagedData'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in averagedFrame:\n",
    "    temp=averagedFrame[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './expirementData'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in expirementFrame:\n",
    "    temp=expirementFrame[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  },
  {
   "source": [
    "### Increase the Resolution\n",
    "pad out the dataframes to have values for every second."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "stretchedDF = {}\n",
    "for i in averagedFrame:\n",
    "    tempFrame = averagedFrame[i].values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    stretchedDF[i] = pd.DataFrame(tempList, columns = expirementFrame[list(expirementFrame.keys())[0]].columns)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "stretchExpDf = {}\n",
    "for i in expirementFrame:\n",
    "    tempFrame = expirementFrame[i].values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    stretchExpDf[i] = pd.DataFrame(tempList, columns = expirementFrame[list(expirementFrame.keys())[0]].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './stretchedAvgData'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in stretchedDF:\n",
    "    temp=stretchedDF[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './stretchedExpirementData'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in stretchExpDf:\n",
    "    temp=stretchExpDf[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  }
 ]
}