{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "metadata": {
   "interpreter": {
    "hash": "5751d7c496c272386542b26abd80dd065779a39feb74bf22f2b4e5e7f629ef29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### General Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "import os\n",
    "import glob\n",
    "from cleanUp import cleanUp\n",
    "from fillDf import fillDf\n",
    "from fixYearStamp import fixYearStamp\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "source": [
    "### Data Cleaning\n",
    "Passing the sensor data through the cleanUp function to get fix timestamps and delete null timestamps."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_csv_files = sorted(glob.glob(\"./Data/*.txt\"))\n",
    "# insert the desired start time\n",
    "cutOffTime = '4/19/2021 17:00:00'\n",
    "endTime = '4/19/2021 20:00:00'\n",
    "# insert the time rectifying offsets. default of for nothing {'':0}\n",
    "sensorConditions = {'S-01':7,'S-02':7,'S-03':7,'S-04':7,'S-05':7,'S-06':7,'S-15':7,'S-19':7}\n",
    "#This indicates which columns to keep. Here we're taking all of the dP info and the timestamps\n",
    "columns = [0,1,6,7,8,9,10,11]\n",
    "# Here are obversed timestamps that need to removed from the data\n",
    "badTimes = ['     0/0/0      0:0:0','2165/165/165 165:165:85']\n",
    "# Controls wether zones will be created automatically or by k-means clusters\n",
    "ZoneAutomation = False\n",
    "# Sensors to exclude from zone\n",
    "outdoorSensors = ['S-16','S-17','S-18','S-19']\n",
    "# Controls the binning\n",
    "numberOfZones = 3\n",
    "# 10s of seconds before nebulization to include in the expirement csv files\n",
    "preCursorFactor = 6\n",
    "# which particle to analyze\n",
    "particle = 'Dp>0.3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = '4/19/2021'\n",
    "expTRange = {\n",
    "\n",
    "    'OR 5 Unblocked':\n",
    "    [\n",
    "    pd.Timestamp(day + ' 5:23:24 PM'),\n",
    "    pd.Timestamp(day + ' 5:32:20 PM'),\n",
    "    pd.Timestamp(day + ' 5:42:00 PM'),\n",
    "    pd.Timestamp(day + ' 5:52:00 PM'),\n",
    "    pd.Timestamp(day + ' 5:58:00 PM'),\n",
    "    pd.Timestamp(day + ' 6:25:20 PM')],\n",
    "    'OR 5 Blocked':\n",
    "    [\n",
    "    pd.Timestamp(day + ' 6:08:50 PM'),\n",
    "    pd.Timestamp(day + ' 6:16:50 PM'),\n",
    "    pd.Timestamp(day + ' 6:25:20 PM')],\n",
    "    'OR 12 Unblocked':\n",
    "    [\n",
    "    pd.Timestamp(day + ' 6:52:50 PM'),\n",
    "    pd.Timestamp(day + ' 7:03:30 PM'),\n",
    "    pd.Timestamp(day + ' 7:13:30 PM')],\n",
    "    'OR 12 Blocked':\n",
    "    [\n",
    "    pd.Timestamp(day + ' 7:25:24 PM'),\n",
    "    pd.Timestamp(day + ' 7:34:45 PM'),\n",
    "    pd.Timestamp(day + ' 7:38:24 PM')],\n",
    "}\n",
    "\n",
    "#enter in the expirement length as seconds/10\n",
    "expTLen = {\n",
    "    'OR 5 Unblocked' : 5*6,\n",
    "    'OR 5 Blocked':5*6,\n",
    "    'OR 12 Unblocked':8*6,\n",
    "    'OR 12 Blocked':7*6,\n",
    "}\n",
    "# Manual Zone set up\n",
    "zoneList = {\n",
    "    'Zone 1' : ['S-01','S-02','S-03','S-04','S-05','S-06'],\n",
    "    'Zone 2' : ['S-7','S-8','S-9','S-10''S-11','S-12','S-13','S-14'],\n",
    "    'Zone 3' : ['S-15','S-18','S-19']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['./Data\\\\S-01.txt',\n",
       " './Data\\\\S-02.txt',\n",
       " './Data\\\\S-03.txt',\n",
       " './Data\\\\S-04.txt',\n",
       " './Data\\\\S-05.txt',\n",
       " './Data\\\\S-06.txt',\n",
       " './Data\\\\S-07.txt',\n",
       " './Data\\\\S-08.txt',\n",
       " './Data\\\\S-09.txt',\n",
       " './Data\\\\S-10.txt',\n",
       " './Data\\\\S-11.txt',\n",
       " './Data\\\\S-12.txt',\n",
       " './Data\\\\S-13.txt',\n",
       " './Data\\\\S-14.txt',\n",
       " './Data\\\\S-15.txt',\n",
       " './Data\\\\S-16.txt',\n",
       " './Data\\\\S-18.txt',\n",
       " './Data\\\\S-19.txt']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "all_csv_files"
   ]
  },
  {
   "source": [
    "Changed this to markdown so it won't run twice, had to fix the timestamps on S-12\n",
    "filePath        = all_csv_files[11]\n",
    "incorrectString = '21/3/22'\n",
    "date            = '3/22/2021'\n",
    "charTimeStart   = 11\n",
    "charTimeEnd     = 21\n",
    "offset          = 0\n",
    "fixYearStamp(filePath,incorrectString,date,charTimeStart,charTimeEnd,offset)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "S-01     2021-04-19 17:22:10      2021-04-19 19:58:59       mod: yes\nS-02     2021-04-19 17:03:05      2021-04-19 19:59:29       mod: yes\nS-03     2021-04-19 17:19:10      2021-04-19 19:59:09       mod: yes\nS-04     2021-04-19 17:39:10      2021-04-19 19:58:59       mod: yes\nS-05     2021-04-19 17:06:10      2021-04-19 19:58:29       mod: yes\nS-06     2021-04-19 17:11:10      2021-04-19 19:59:09       mod: yes\nS-07     2021-04-19 17:07:34      2021-04-19 19:59:18       mod: no\nS-08     2021-04-19 17:05:08      2021-04-19 19:59:11       mod: no\nS-09     2021-04-19 17:10:11      2021-04-19 19:58:58       mod: no\nS-10     2021-04-19 17:09:59      2021-04-19 19:58:36       mod: no\nS-11     2021-04-19 17:19:22      2021-04-19 19:59:09       mod: no\nS-12     2021-04-19 17:11:50      2021-04-19 19:58:57       mod: no\nS-13     2021-04-19 17:21:07      2021-04-19 19:59:14       mod: no\nS-14     2021-04-19 17:05:18      2021-04-19 19:59:15       mod: no\nS-15     2021-04-19 17:12:10      2021-04-19 20:01:55       mod: yes\nS-16     2021-04-19 17:07:36      2021-04-19 19:58:50       mod: no\nS-18     2021-04-19 17:11:07      2021-04-19 20:02:02       mod: no\nS-19     2021-04-19 17:10:16      2021-04-19 20:02:01       mod: yes\n"
     ]
    }
   ],
   "source": [
    "data = cleanUp(cutOffTime,sensorConditions,all_csv_files,columns,badTimes)"
   ]
  },
  {
   "source": [
    "### Exporting Data\n",
    "Here we can export the organized data frames as csv files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './proccessedData'\n",
    "for x in data:\n",
    "    temp=data[x]\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  },
  {
   "source": [
    "### Checking Data\n",
    "Here we scan through the data for irregularities in data recording."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.21 % potential error in  S-01\n",
      "   9  20\n",
      "   1   1\n",
      "\n",
      "0.09 % potential error in  S-02\n",
      "  14\n",
      "   1\n",
      "\n",
      "0.21 % potential error in  S-03\n",
      "   9  20\n",
      "   1   1\n",
      "\n",
      "0.6 % potential error in  S-04\n",
      "  11   9\n",
      "   2   3\n",
      "\n",
      "0.68 % potential error in  S-05\n",
      "  16   9  40  30  20   4\n",
      "   1   1   1   1   2   1\n",
      "\n",
      "0.2 % potential error in  S-06\n",
      "   9  20\n",
      "   1   1\n",
      "\n",
      "19.9 % potential error in  S-07\n",
      "  27 627  20\n",
      "   1   1 159\n",
      "\n",
      "49.86 % potential error in  S-08\n",
      "  59  27  20\n",
      "   1   2 343\n",
      "\n",
      "50.0 % potential error in  S-09\n",
      "  26  21  22  18  19  20  11\n",
      "   1   2   1   1   2 330   1\n",
      "\n",
      "49.85 % potential error in  S-10\n",
      "  17  20\n",
      "   1 336\n",
      "\n",
      "19.88 % potential error in  S-11\n",
      "  27  20\n",
      "   1 158\n",
      "\n",
      "99.8 % potential error in  S-12\n",
      "  27  20\n",
      "   1 500\n",
      "\n",
      "20.08 % potential error in  S-13\n",
      "  23  19  20  58  27\n",
      "   1   1 154   1   1\n",
      "\n",
      "49.92 % potential error in  S-14\n",
      " 939  21  20  27\n",
      "   1   1 314   1\n",
      "\n",
      "0.49 % potential error in  S-15\n",
      "   9  17  13  15  11\n",
      "   1   1   1   1   1\n",
      "\n",
      "0.1 % potential error in  S-16\n",
      "  14\n",
      "   1\n",
      "\n",
      "0.39 % potential error in  S-18\n",
      "   9  14  17  15\n",
      "   1   1   1   1\n",
      "\n",
      "0.1 % potential error in  S-19\n",
      "  15\n",
      "   1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "directory = './dataInfo'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "fout = open('./dataInfo/time_Frequency_Error_Log.txt','wt')\n",
    "errors = {}\n",
    "errorCount = {}\n",
    "# Enter the expected interval here\n",
    "interval = 10\n",
    "for x in data:\n",
    "    # errors keeps track of length of each time interval error that occurs\n",
    "    errors[x] = set(())\n",
    "    # errorCount keeps track of how many times each time interval error occured\n",
    "    errorCount[x] = {}\n",
    "    # counter keeps track of the total time interval errors per sensor\n",
    "    counter = 0\n",
    "    #shows the total\n",
    "    temp = data[x]\n",
    "    for idx,i in enumerate(temp['Date_Time']):\n",
    "        try:\n",
    "            if not ((temp['Date_Time'][idx+1] - i) == pd.Timedelta(seconds=interval)):\n",
    "                timeErr = temp['Date_Time'][idx+1] - i\n",
    "                if str(timeErr.seconds) in errorCount[x]:\n",
    "                    errorCount[x][str(timeErr.seconds)] +=1\n",
    "                else:\n",
    "                    errorCount[x][str(timeErr.seconds)] = 1\n",
    "\n",
    "                errors[x].add(timeErr)\n",
    "\n",
    "\n",
    "                counter += 1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(str(round(counter/len(temp)*100,2)),'% potential error in ', x)\n",
    "    fout.write('potential error in '+ x +'\\n' + str(round(counter/len(temp)*100,2))+'%'+'\\n')\n",
    "\n",
    "    # display the different types of errors\n",
    "    lst = [i.seconds for i in errors[x]]\n",
    "    frmt = \"{:>4}\"*len(lst)\n",
    "    print(frmt.format(*lst))\n",
    "    fout.write(\"Time Errors\" + frmt.format(*lst)+ '\\n')\n",
    "\n",
    "    # display the quantity of each type of error\n",
    "    lst = [errorCount[x][str(i.seconds)] for i in errors[x]]\n",
    "    frmt = \"{:>4}\"*len(lst)\n",
    "    print(frmt.format(*lst))\n",
    "    fout.write(\"# Observed \" + frmt.format(*lst)+ '\\n')\n",
    "\n",
    "    print()\n",
    "    fout.write('\\n')\n",
    "\n",
    "\n",
    "fout.close()"
   ]
  },
  {
   "source": [
    "Notice there are quite a few repeating errors here in our data set. We can either choose to interpolate the data inbetween or pad it with 0s. For gaps <40s i will interpolate, but for gaps >40 i will 0 pad."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "S-01   ['% of values from interpolation : 0.186', '% of values from 0-padding : 12.477', '% of values not changed : 87.337']\n",
      "S-02   ['% of values from interpolation : 0.0', '% of values from 0-padding : 1.764', '% of values not changed : 98.236']\n",
      "S-03   ['% of values from interpolation : 0.186', '% of values from 0-padding : 10.791', '% of values not changed : 89.023']\n",
      "S-04   ['% of values from interpolation : 0.372', '% of values from 0-padding : 21.974', '% of values not changed : 77.654']\n",
      "S-05   ['% of values from interpolation : 1.027', '% of values from 0-padding : 3.548', '% of values not changed : 95.425']\n",
      "S-06   ['% of values from interpolation : 0.186', '% of values from 0-padding : 6.326', '% of values not changed : 93.488']\n",
      "S-07   ['% of values from interpolation : 29.74', '% of values from 0-padding : 10.13', '% of values not changed : 60.13']\n",
      "S-08   ['% of values from interpolation : 64.312', '% of values from 0-padding : 3.439', '% of values not changed : 32.249']\n",
      "S-09   ['% of values from interpolation : 62.663', '% of values from 0-padding : 5.773', '% of values not changed : 31.564']\n",
      "S-10   ['% of values from interpolation : 62.873', '% of values from 0-padding : 5.597', '% of values not changed : 31.53']\n",
      "S-11   ['% of values from interpolation : 29.581', '% of values from 0-padding : 10.884', '% of values not changed : 59.535']\n",
      "S-12   ['% of values from interpolation : 93.296', '% of values from 0-padding : 6.704', '% of values not changed : 0.0']\n",
      "S-13   ['% of values from interpolation : 29.275', '% of values from 0-padding : 12.361', '% of values not changed : 58.364']\n",
      "S-14   ['% of values from interpolation : 58.829', '% of values from 0-padding : 11.71', '% of values not changed : 29.461']\n",
      "S-15   ['% of values from interpolation : 0.185', '% of values from 0-padding : 6.852', '% of values not changed : 92.963']\n",
      "S-16   ['% of values from interpolation : 0.186', '% of values from 0-padding : 4.283', '% of values not changed : 95.531']\n",
      "S-18   ['% of values from interpolation : 0.37', '% of values from 0-padding : 6.204', '% of values not changed : 93.426']\n",
      "S-19   ['% of values from interpolation : 0.185', '% of values from 0-padding : 5.741', '% of values not changed : 94.074']\n"
     ]
    }
   ],
   "source": [
    "fout = open('./dataInfo/interpolation_Effect_Log.txt','wt')\n",
    "interpDF = {}\n",
    "\n",
    "for x in data:\n",
    "    df = data[x]\n",
    "    cutoff = 40\n",
    "    freq = '10S'\n",
    "    try:\n",
    "        interpDF[x],accuracy = fillDf(df,freq,cutOffTime,endTime,cutoff)\n",
    "        print(x,' ',accuracy)\n",
    "        fout.write(x+' '+ '\\n' + accuracy[0]+ '\\n'+ accuracy[1]+ '\\n'+ accuracy[2] +'\\n\\n')\n",
    "    except IndexError:\n",
    "        print(x,'NO DATA')\n",
    "        fout.write(x+'NO DATA'+'\\n')\n",
    "fout.close()        "
   ]
  },
  {
   "source": [
    "### Export Data\n",
    "export the newly interpolated data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './interpolatedData'\n",
    "for x in interpDF:\n",
    "    temp=interpDF[x]\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  },
  {
   "source": [
    "### Merge the DataFrames"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4 1071\n"
     ]
    }
   ],
   "source": [
    "length = []\n",
    "for x in interpDF:\n",
    "    length.append(len(interpDF[x]))\n",
    "index = min(length)\n",
    "lowIDX,lowValue = [[i,value] for i,value in enumerate(length) if value == index][0]\n",
    "print(lowIDX,lowValue)"
   ]
  },
  {
   "source": [
    "for count,key in enumerate(list(interpDF.keys())):\n",
    "    print(count+1,key,temp[count+1])"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               Date_Time  S-01  S-02  S-03  S-04  S-05  S-06  S-07  S-08  \\\n",
       "0    2021-04-19 17:00:00     0     0     0     0     0     0     0     0   \n",
       "1    2021-04-19 17:00:10     0     0     0     0     0     0     0     0   \n",
       "2    2021-04-19 17:00:20     0     0     0     0     0     0     0     0   \n",
       "3    2021-04-19 17:00:30     0     0     0     0     0     0     0     0   \n",
       "4    2021-04-19 17:00:40     0     0     0     0     0     0     0     0   \n",
       "...                  ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "1066 2021-04-19 19:57:40    51    96    81    69   138    84    90    18   \n",
       "1067 2021-04-19 19:57:50    72   114   129    69   108    72    63    18   \n",
       "1068 2021-04-19 19:58:00    57    81   105    69   114    21   108    57   \n",
       "1069 2021-04-19 19:58:10   153    54   114    60    84    84   108   129   \n",
       "1070 2021-04-19 19:58:20   144   138    66    42    66    90   117   129   \n",
       "\n",
       "      S-09  ...  S-11  S-12  S-13  S-14  S-15  S-16  S-18  S-19    Average  \\\n",
       "0        0  ...     0     0     0     0     0     0     0     0   0.000000   \n",
       "1        0  ...     0     0     0     0     0     0     0     0   0.000000   \n",
       "2        0  ...     0     0     0     0     0     0     0     0   0.000000   \n",
       "3        0  ...     0     0     0     0     0     0     0     0   0.000000   \n",
       "4        0  ...     0     0     0     0     0     0     0     0   0.000000   \n",
       "...    ...  ...   ...   ...   ...   ...   ...   ...   ...   ...        ...   \n",
       "1066    94  ...   198    81    63   154     9   102    33    18  81.111111   \n",
       "1067    84  ...   117   105   126   144    30    96    54    27  83.833333   \n",
       "1068    84  ...    81   129   126   144    48    84     0    39  78.222222   \n",
       "1069    79  ...    81   121   157   126    60    51     0    48  86.666667   \n",
       "1070   150  ...    64   114   150    90    51    72    63    78  93.055556   \n",
       "\n",
       "         Variance  \n",
       "0        0.000000  \n",
       "1        0.000000  \n",
       "2        0.000000  \n",
       "3        0.000000  \n",
       "4        0.000000  \n",
       "...           ...  \n",
       "1066  2221.432099  \n",
       "1067  1273.472222  \n",
       "1068  1404.506173  \n",
       "1069  1622.888889  \n",
       "1070  1302.719136  \n",
       "\n",
       "[1071 rows x 21 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date_Time</th>\n      <th>S-01</th>\n      <th>S-02</th>\n      <th>S-03</th>\n      <th>S-04</th>\n      <th>S-05</th>\n      <th>S-06</th>\n      <th>S-07</th>\n      <th>S-08</th>\n      <th>S-09</th>\n      <th>...</th>\n      <th>S-11</th>\n      <th>S-12</th>\n      <th>S-13</th>\n      <th>S-14</th>\n      <th>S-15</th>\n      <th>S-16</th>\n      <th>S-18</th>\n      <th>S-19</th>\n      <th>Average</th>\n      <th>Variance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-04-19 17:00:00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2021-04-19 17:00:10</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2021-04-19 17:00:20</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2021-04-19 17:00:30</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2021-04-19 17:00:40</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1066</th>\n      <td>2021-04-19 19:57:40</td>\n      <td>51</td>\n      <td>96</td>\n      <td>81</td>\n      <td>69</td>\n      <td>138</td>\n      <td>84</td>\n      <td>90</td>\n      <td>18</td>\n      <td>94</td>\n      <td>...</td>\n      <td>198</td>\n      <td>81</td>\n      <td>63</td>\n      <td>154</td>\n      <td>9</td>\n      <td>102</td>\n      <td>33</td>\n      <td>18</td>\n      <td>81.111111</td>\n      <td>2221.432099</td>\n    </tr>\n    <tr>\n      <th>1067</th>\n      <td>2021-04-19 19:57:50</td>\n      <td>72</td>\n      <td>114</td>\n      <td>129</td>\n      <td>69</td>\n      <td>108</td>\n      <td>72</td>\n      <td>63</td>\n      <td>18</td>\n      <td>84</td>\n      <td>...</td>\n      <td>117</td>\n      <td>105</td>\n      <td>126</td>\n      <td>144</td>\n      <td>30</td>\n      <td>96</td>\n      <td>54</td>\n      <td>27</td>\n      <td>83.833333</td>\n      <td>1273.472222</td>\n    </tr>\n    <tr>\n      <th>1068</th>\n      <td>2021-04-19 19:58:00</td>\n      <td>57</td>\n      <td>81</td>\n      <td>105</td>\n      <td>69</td>\n      <td>114</td>\n      <td>21</td>\n      <td>108</td>\n      <td>57</td>\n      <td>84</td>\n      <td>...</td>\n      <td>81</td>\n      <td>129</td>\n      <td>126</td>\n      <td>144</td>\n      <td>48</td>\n      <td>84</td>\n      <td>0</td>\n      <td>39</td>\n      <td>78.222222</td>\n      <td>1404.506173</td>\n    </tr>\n    <tr>\n      <th>1069</th>\n      <td>2021-04-19 19:58:10</td>\n      <td>153</td>\n      <td>54</td>\n      <td>114</td>\n      <td>60</td>\n      <td>84</td>\n      <td>84</td>\n      <td>108</td>\n      <td>129</td>\n      <td>79</td>\n      <td>...</td>\n      <td>81</td>\n      <td>121</td>\n      <td>157</td>\n      <td>126</td>\n      <td>60</td>\n      <td>51</td>\n      <td>0</td>\n      <td>48</td>\n      <td>86.666667</td>\n      <td>1622.888889</td>\n    </tr>\n    <tr>\n      <th>1070</th>\n      <td>2021-04-19 19:58:20</td>\n      <td>144</td>\n      <td>138</td>\n      <td>66</td>\n      <td>42</td>\n      <td>66</td>\n      <td>90</td>\n      <td>117</td>\n      <td>129</td>\n      <td>150</td>\n      <td>...</td>\n      <td>64</td>\n      <td>114</td>\n      <td>150</td>\n      <td>90</td>\n      <td>51</td>\n      <td>72</td>\n      <td>63</td>\n      <td>78</td>\n      <td>93.055556</td>\n      <td>1302.719136</td>\n    </tr>\n  </tbody>\n</table>\n<p>1071 rows × 21 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "columns = list(interpDF.keys())\n",
    "mergedData = pd.DataFrame({'Date_Time':interpDF[columns[lowIDX]]['Date_Time']})\n",
    "for idx,column in enumerate(columns):\n",
    "    mergedData[column] = interpDF[column][particle]\n",
    "Average = np.mean(mergedData,axis=1)\n",
    "Variance = np.var(mergedData,axis=1)\n",
    "mergedData['Average'] = Average\n",
    "mergedData['Variance'] = Variance\n",
    "mergedData"
   ]
  },
  {
   "source": [
    "### Increase Resolution on mergedData"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in mergedData:\n",
    "    tempFrame = mergedData.values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    hiResMergedDF = pd.DataFrame(tempList, columns = mergedData.keys())"
   ]
  },
  {
   "source": [
    "### Export Merged Frames"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './mergedData/'\n",
    "if not os.path.exists(directory):\n",
    "\n",
    "    os.makedirs(directory)\n",
    "\n",
    "location = os.path.join(directory+'mergedFrame.csv')\n",
    "hiResMergedDF.to_csv(location,index=False)"
   ]
  },
  {
   "source": [
    "### Create csv files for each animation\n",
    "We have 3 expirements in each that we want to average across the range"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mergedData = pd.read_csv('./mergedData/mergedFrame.csv',parse_dates=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time = mergedData['Date_Time']\n",
    "expIndexes = {}\n",
    "for i in expTRange:\n",
    "    expIndexes[i] = []\n",
    "    for x in expTRange[i]:\n",
    "        for start,n in enumerate(time):\n",
    "           if n >= x:\n",
    "               expIndexes[i].append(start)\n",
    "               break"
   ]
  },
  {
   "source": [
    "## Determining Zones\n",
    "Here we first create 'averagedFrame's. These are dictionaries that at each 'label' (which corresponds to the name of an expirement) we have a pandas dataframe containing the results of all of the trails in an expirement summed, and then divided by the total number of trails.\n",
    "Anytime you are adjusting the Zones, everything below here must be run. The values of many of these DataFrames are mutated"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preCursorFactor is defined at the start\n",
    "averagedFrame = {}\n",
    "expirementFrame = {}\n",
    "\n",
    "for label in expIndexes:\n",
    "    runSumFrames = expIndexes[label][0]-expIndexes[label][0]\n",
    "    for idx,time in enumerate(expIndexes[label]):\n",
    "        start = expIndexes[label][idx] - preCursorFactor\n",
    "        end = expIndexes[label][idx] + expTLen[label]\n",
    "        expirementFrame[label+' Exp '+str(idx+1)] = mergedData.iloc[ start : end , 1: ].reset_index(drop = True)\n",
    "        runSumFrames += expirementFrame[label+' Exp '+str(idx+1)]\n",
    "        \n",
    "    averagedFrame[label] = runSumFrames/(idx+1)"
   ]
  },
  {
   "source": [
    "Calculating the correct Zones for each expirement"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ZoneAutomation:\n",
    "    # numberOfZones is defined at the start\n",
    "    ZoneAssignments = {}\n",
    "    for frame in averagedFrame:\n",
    "        # at this point averagedFrame should just be the averaged sum of the expirementFrame trails. Last two columns are overall average and varaince so they should be ignored.\n",
    "        avgFrm = averagedFrame[frame]\n",
    "        # outdoorSensors must have its spelling exactly match\n",
    "        columns = list(set(avgFrm.keys()[:-2])- set(outdoorSensors))\n",
    "        columns.sort()\n",
    "\n",
    "        X = {}\n",
    "        for column in columns:\n",
    "            value,index = max([(value,index) for index,value in enumerate(avgFrm[column])]) \n",
    "            X[column] = np.array([np.log(value),index])\n",
    "        X = [X[i] for i in X]\n",
    "        kmeans = KMeans(n_clusters=numberOfZones,random_state=0).fit(X)\n",
    "        ZoneAssignments[frame] = kmeans.labels_\n",
    "    z = numberOfZones\n",
    "    ZDf = pd.DataFrame(ZoneAssignments)\n",
    "    ZDf = ZDf.append(pd.DataFrame([[z]*len(outdoorSensors)]*len(expIndexes),columns = ZoneAssignments.keys()),ignore_index=True)\n",
    "    ZoneAssignments = ZDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ZoneAutomation:\n",
    "    ZoneAssignments = {}\n",
    "    for frame in averagedFrame:\n",
    "        # at this point averagedFrame should just be the averaged sum of the expirementFrame trails. Last two columns are overall average and varaince so they should be ignored.\n",
    "        avgFrm = averagedFrame[frame]\n",
    "        # outdoorSensors must have its spelling exactly match\n",
    "        columns = list(set(avgFrm.keys()[:-2])- set(outdoorSensors))\n",
    "        columns.sort()\n",
    "        ZoneAssignments[frame] = []\n",
    "        for value,zone in enumerate(zoneList):\n",
    "            ZoneAssignments[frame].extend([value for i in range(len(zone))])\n",
    "    ZDf = pd.DataFrame(ZoneAssignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './dataInfo'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "location = os.path.join(directory,'ZoneAssignments.csv')\n",
    "ZDf.to_csv(location,index=False)"
   ]
  },
  {
   "source": [
    "Zoning the expirement data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "zonedAvgFrame = {}\n",
    "for key in ZoneAssignments:\n",
    "    occourances = [list(ZoneAssignments[key]).count(x) for x in set(ZoneAssignments[key])]\n",
    "    zoneRunSum = [0]*numberOfZones\n",
    "    zonedAvgFrame[key] = averagedFrame[key]\n",
    "    for idx,column in enumerate(columns):\n",
    "        zoneRunSum[ZoneAssignments[key][idx]] += zonedAvgFrame[key][column]\n",
    "    for idx in range(numberOfZones):\n",
    "        zonedAvgFrame[key]['Zone '+str(idx+1)] = zoneRunSum[idx]/occourances[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relies on columns still being the values of S-01 - last sensor\n",
    "\n",
    "# Declare an empty dictionary for storing the averaged data for each expirement at the end\n",
    "zonedExpFrame = {}\n",
    "# create a list of all of the various dict keys in expirementFrame so that we can iterate through them to get the data\n",
    "labels = list(expirementFrame.keys())\n",
    "# Take the labels list and remove the Exp # from it, so that now we have a list of keys that we can use to correctly save to create correctly corresponding keys for a dictionary that will store the averages\n",
    "keyList = [x.split(' Exp')[0] for x in labels]\n",
    "\n",
    "for index,exp in enumerate(labels):\n",
    "    # set the key variable to correspond to the exp variable\n",
    "    key = keyList[index]\n",
    "    # Create a runnning sum to keep track of the values\n",
    "    zoneRunSum = [0]*numberOfZones\n",
    "    # set the give the zoneExpFrame the same \n",
    "    zonedExpFrame[exp] = expirementFrame[exp]\n",
    "    occourances = [list(ZoneAssignments[key]).count(x) for x in set(ZoneAssignments[key])]\n",
    "    for idx,column in enumerate(columns):\n",
    "        zoneRunSum[ZoneAssignments[key][idx]] += zonedAvgFrame[key][column]\n",
    "    for idx in range(numberOfZones):\n",
    "        zonedExpFrame[exp]['Zone '+str(idx+1)] = zoneRunSum[idx]/occourances[idx]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './averagedData'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in averagedFrame:\n",
    "    temp=averagedFrame[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './expirementData'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in expirementFrame:\n",
    "    temp=expirementFrame[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  },
  {
   "source": [
    "### Increase the Resolution\n",
    "pad out the dataframes to have values for every second."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stretchedDF = {}\n",
    "for i in averagedFrame:\n",
    "    tempFrame = averagedFrame[i].values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    stretchedDF[i] = pd.DataFrame(tempList, columns = expirementFrame[list(expirementFrame.keys())[0]].columns)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stretchExpDf = {}\n",
    "for i in expirementFrame:\n",
    "    tempFrame = expirementFrame[i].values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    stretchExpDf[i] = pd.DataFrame(tempList, columns = expirementFrame[list(expirementFrame.keys())[0]].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './stretchedAvgData'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in stretchedDF:\n",
    "    temp=stretchedDF[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './stretchedExpirementData'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in stretchExpDf:\n",
    "    temp=stretchExpDf[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  }
 ]
}